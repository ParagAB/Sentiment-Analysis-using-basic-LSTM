{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZyO4Jg3EWZr"
   },
   "source": [
    "# Sentiment Analysis using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xs6nn93pEkCw"
   },
   "source": [
    "### Sentiment analysis can be thought of as the exercise of taking a sentence, paragraph, document, or any piece of natural language, and determining whether that text's emotional tone is positive or negative. We will be applying the concepts of natural language processing to identify the sentiment of the given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTCjmjtXMz74"
   },
   "source": [
    "We'll authenticate the google drive and create connection with google colaboratory.\n",
    "\n",
    "if pydrive is not installed then install pydrive with command\n",
    "\n",
    "**!pip install pydrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "NufF8T8TZmU5",
    "outputId": "5e4078cf-8e6d-444d-e402-ab06d7436c20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
      "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W81g-4pixEFD"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "import tensorflow as tf\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "shk7ZfQ0PbE9"
   },
   "source": [
    "### Make the parent folder shareable and write the id of parent file to get all the files from the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z7C3sAh4xg6U",
    "outputId": "252b3bf8-2a2a-44bf-9a17-f6cc207c2b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: aclImdb.zip, id: 1jGazigaA-9HD91j6YMOn4rGDqu8YKDDj\n"
     ]
    }
   ],
   "source": [
    "file_list = drive.ListFile({'q': \"'189H9l-4H8q_EYUfyIQSvzl9lCXngGJDJ' in parents and trashed=false\"}).GetList()\n",
    "for file1 in file_list:\n",
    "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAmSBqRAhiQJ"
   },
   "outputs": [],
   "source": [
    "zip_file = drive.CreateFile({'id': '1jGazigaA-9HD91j6YMOn4rGDqu8YKDDj'})\n",
    "zip_file.GetContentFile('aclImdb.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-rMEVJGhnth"
   },
   "outputs": [],
   "source": [
    "!unzip 'aclImdb.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QIl3yOst84ZN"
   },
   "source": [
    "## Loading Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ah3XqqBeP-Wv"
   },
   "source": [
    "For simplicity, we're going to use a pretrained word vector model. As one of the biggest players in the ML game, Google was able to train a Word2Vec model on a massive Google News dataset that contained over 100 billion different words! \n",
    "We'd use those vectors, but since the word vectors matrix is quite large, we'll use a much smaller matrix that is trained using GloVe. The matrix will contain 400,000 word vectors, each with a dimensionality of 50. \n",
    "\n",
    "We're going to be importing two different data structures, one will be a Python list with the 400,000 words, and one will be a 400,000 x 50 dimensional embedding matrix that holds all of the word vector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "zq0x7kSFxkwg",
    "outputId": "2fbe2856-8391-46f1-c0b2-abfd5b03cda8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('aclImdb/wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('aclImdb/wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dRg7I71Q-58"
   },
   "source": [
    "Let's take a look at the dimentionality of vocabulary list and word embedding matrix size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "YHSjY4JPx8LQ",
    "outputId": "be748a5a-7bad-428c-82b7-a2d1b80a9ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bvVyDvkQYzHk"
   },
   "source": [
    "We can see how a word is represented in the word list.\n",
    "Lets check the check the word vector for 'happy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "n9_HjD6_x995",
    "outputId": "abc37269-3fb9-443c-86e7-830c594be623"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.092086,  0.2571  , -0.58693 , -0.37029 ,  1.0828  , -0.55466 ,\n",
       "       -0.78142 ,  0.58696 , -0.58714 ,  0.46318 , -0.11267 ,  0.2606  ,\n",
       "       -0.26928 , -0.072466,  1.247   ,  0.30571 ,  0.56731 ,  0.30509 ,\n",
       "       -0.050312, -0.64443 , -0.54513 ,  0.86429 ,  0.20914 ,  0.56334 ,\n",
       "        1.1228  , -1.0516  , -0.78105 ,  0.29656 ,  0.7261  , -0.61392 ,\n",
       "        2.4225  ,  1.0142  , -0.17753 ,  0.4147  , -0.12966 , -0.47064 ,\n",
       "        0.3807  ,  0.16309 , -0.323   , -0.77899 , -0.42473 , -0.30826 ,\n",
       "       -0.42242 ,  0.055069,  0.38267 ,  0.037415, -0.4302  , -0.39442 ,\n",
       "        0.10511 ,  0.87286 ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('happy')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYOYEEHa882j"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgSPpRdS8ZZF"
   },
   "source": [
    "There are total 25000 movie reviews for training, 12500 reviews are positive and 12500 negative reviews. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. Each of the reviews is stored in a txt file that we need to parse through. The positive reviews are stored in one directory and the negative reviews are stored in another. The following piece of code will determine total and average number of words in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "yL3Ptu58x_g7",
    "outputId": "df4b6cd5-a544-43df-c86c-9e0dce1d45a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['aclImdb/train/pos/'\n",
    "                 + f for f in listdir('aclImdb/train/pos')\n",
    "                 if isfile(join('aclImdb/train/pos', f))]\n",
    "\n",
    "# positiveFiles_test = ['aclImdb/test/pos/'\n",
    "#                  + f for f in listdir('aclImdb/test/pos')\n",
    "#                  if isfile(join('aclImdb/test/pos', f))]\n",
    "\n",
    "# total_pos = positiveFiles + positiveFiles_test\n",
    "\n",
    "negativeFiles = ['aclImdb/train/neg/'\n",
    "                 + f for f in listdir('aclImdb/train/neg/') \n",
    "                 if isfile(join('aclImdb/train/neg/', f))]\n",
    "\n",
    "# negativeFiles_test = ['aclImdb/test/neg/'\n",
    "#                  + f for f in listdir('aclImdb/test/neg/') \n",
    "#                  if isfile(join('aclImdb/test/neg/', f))]\n",
    "\n",
    "# total_neg =  negativeFiles_test + negativeFiles\n",
    "\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "GcXFyVnwyLx8",
    "outputId": "53e8ab40-b23d-493c-f4cd-dfa4876c8ab2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGFJJREFUeJzt3X2UXXV97/H3kAeNIchDR4MREVbx\n26tevZYFFEMkPKk8pHQZKHeRIiR0iYpdBR9uQ6HIo1BbtFeh17IgPGktklsKLC3QBKoJKA1W8aLc\nr6YIaollWk2MlTuQZO4few9zmGQye7LPnpkz5/1aaxbn/M4+53zPL8N8zm//9v7tnoGBASRJqmO3\niS5AktT5DBNJUm2GiSSpNsNEklSbYSJJqm36RBfQTlu2bB34+c9/NdFlTAp77fUK7IuCfTHEvhhi\nXwzp7Z3TU/c1ptTIZPr0aRNdwqRhXwyxL4bYF0Psi/aaUmEiSZoYhokkqTbDRJJUm2EiSarNMJEk\n1WaYSJJqM0wkSbUZJpKk2gwTSVJthokkqTbDRJJUm2EiSarNMJEk1WaYSJJqM0wkSbU1dnGsiFgI\n3AF8t2z6P8AngduAacAG4IzM7I+IJcB5wDbg+sy8MSJmADcD+wNbgaWZ+WRT9UqSdl3TV1r8amae\nMngnIm4CrsvMOyLiE8CyiLgVuBg4FHgeWBcRdwKLgI2ZuSQi3glcBZzWcL1ttezqBypvu2L50Q1W\nIknNGu/dXAuBu8vb9wDHAocB6zJzU2Y+BzwEzAeOAe4st11VtkmSJqGmRyZvjIi7gb2BS4HZmdlf\nPvYssC8wF+hrec527Zm5LSIGImJmZj6/szfs7Z3T5o8wPpqou1P7ogn2xRD7Yoh90T5NhskPKALk\nS8CBwIPD3m+kC9iPtf0l+vo2V61vUml33b29czq2L9rNvhhiXwyxL4a0I1Qb282Vmf+ambdn5kBm\n/gvwU2CviJhVbjIPeKb8mdvy1O3ay8n4ntFGJZKkidFYmETEkoj4aHl7LvBq4CZgcbnJYuBe4BHg\nkIjYMyJ2p5gbWQPcD5xabruIYmQjSZqEmpyAvxs4MiLWAHcBHwAuBM4s2/YGbikn3ZcD91FMtF+a\nmZuA24FpEbEWOBe4oMFaJUk1NDZnkpmbKUYUwx23g21XAiuHtW0FljZTnSSpnTwDXpJUm2EiSarN\nMJEk1WaYSJJqM0wkSbUZJpKk2gwTSVJthokkqTbDRJJUm2EiSarNMJEk1WaYSJJqM0wkSbUZJpKk\n2gwTSVJthokkqTbDRJJUm2EiSarNMJEk1WaYSJJqM0wkSbUZJpKk2gwTSVJthokkqTbDRJJUm2Ei\nSarNMJEk1WaYSJJqM0wkSbUZJpKk2gwTSVJt05t88YiYBTwOXA6sBm4DpgEbgDMysz8ilgDnAduA\n6zPzxoiYAdwM7A9sBZZm5pNN1ipJ2nVNj0wuAn5W3r4MuC4zFwDrgWURMRu4GDgWWAicHxF7A6cD\nGzPzCOBK4KqG65Qk1dBYmETEbwBvBL5cNi0E7i5v30MRIIcB6zJzU2Y+BzwEzAeOAe4st11VtkmS\nJqkmd3NdA3wIOLO8Pzsz+8vbzwL7AnOBvpbnbNeemdsiYiAiZmbm86O9aW/vnDaVP76aqLtT+6IJ\n9sUQ+2KIfdE+jYRJRLwX+Hpm/jAidrRJzwhPHWv7dvr6NlfddFJpd929vXM6ti/azb4YYl8MsS+G\ntCNUmxqZnAgcGBEnAa8F+oFfRsSscnfWPOCZ8mduy/PmAd9oaX+snIzvqTIqkSRNjEbCJDNPG7wd\nEZcATwFvBxYDny//ey/wCHBDROwJbKGYGzkP2AM4FbgPWAQ82ESdkqT2GM/zTD4OnBkRa4C9gVvK\nUcpyitBYBVyamZuA24FpEbEWOBe4YBzrlCSNUaPnmQBk5iUtd4/bweMrgZXD2rYCS5utTJLULp4B\nL0mqrfGRiapZdvUDlbddsfzoBiuRpLFzZCJJqs0wkSTVZphIkmozTCRJtRkmkqTaDBNJUm2GiSSp\nNsNEklSbYSJJqs0wkSTVZphIkmozTCRJtRkmkqTaDBNJUm2GiSSpNsNEklSbYSJJqq1SmERET9OF\nSJI6V9WRydMRcUVEHNhoNZKkjlT1GvCHAqcAKyLiBeAmYGVmPt9YZZKkjlFpZJKZP83MazNzIfCB\n8mdDOVp5eZMFSpImv8oT8BHxjohYAfw98BBwBLARuKOh2iRJHaLSbq6IWA88BVwPnJOZL5QPPRER\nv9NQbZKkDlF1zuTdQE9m/gAgIt6Wmd8qH1vQSGWSpI5RdTfXWcAFLfeXR8TVAJk50O6iJEmdpWqY\nHJWZywbvZOZpFHMmkiRVDpOZETFz8E5E7A7MaKYkSVKnqTpn8jmKyfZHgWnAIcAlTRUlSeoslcIk\nM2+MiH+gCJEB4PzM/PHOnhMRrwBuBl4NvBy4HHgMuI0ikDYAZ2Rmf0QsAc4DtgHXl+83o3z+/sBW\nYGlmPjnmTyhJalzVtbleDrwN2APYEzguIpbt/FksAh7NzCOB3wU+BVwGXJeZC4D1wLKImA1cDBwL\nLATOj4i9gdOBjZl5BHAlcNUYP5skaZxU3c11H8Xo4OmWtgFgxUhPyMzbW+7uB/yEIizeX7bdA3wU\nSGBdZm4CiIiHgPnAMcCt5bardvZekqSJVTVMZpQjjDGLiIeB1wInAasys7986FlgX2Au0NfylO3a\nM3NbRAxExMzR1gPr7Z2zK2V2lKqfsRv6oir7Yoh9McS+aJ+qYfLdiNgnM/9jrG+QmW+PiP8GfB5o\nXcp+pGXtx9r+En19m8dQXWeq8hl7e+d0RV9UYV8MsS+G2BdD2hGqVcPktcD6iHgC2DLYmJnvGOkJ\nEXEw8Gxm/jgzvx0R04HNETErM58D5gHPlD9zW546D/hGS/tj5WR8j6sUS9LkVDVMrt6F134HxZFY\n50XEq4HdgXuBxRSjlMXl/UeAGyJiT4qgmk9xZNcewKkU8zWLgAd3oQZJ0jiougT9VynC4L+Wt38C\nfG2Up30OeFVErAG+DJwLfBw4s2zbG7ilHKUspwiNVcCl5WT87cC0iFhbPveCHbyHJGkSqLpq8J8C\nB1GMNK6lOGz3VcAfjPScMiRO38FDx+1g25XAymFtW4GlVeqTJE2sqsupHJmZ7wF+AZCZlwO/2VhV\nkqSOUjVMniv/OwAQEdOoPt8iSZriqobJwxFxE/CaiPgw8FXgHxurSpLUUapOwF9IMYm+muIw4U9l\n5h81WZgkqXNUnYA/EPjn8ufFNhdelCRB9XmP1ZTzJcDLKI7kepxi8UdJUperugT9Aa33I+JNwNmN\nVCRJ6jhVJ+BfIjO/Cxzc5lokSR2q6pzJZcOa9qO4rokkSZVHJltbfrZQXDHxhKaKkiR1lqoT8Jfv\nqDEidoPieiNtq0ijWnb1A5W3XbH86AYrkaRC1TD5fxTXbR+uh+Iorx09JknqElXD5FLge8D9FOGx\nCDgoM69oqjBJUueoGiZHZ+aVLfdvj4jVgGEiSaocJvtExAkMXcNkAdDbTEmSpE5TNUzeB1wD/E15\n/3Hgg41UJEnqOFXPgP8nYEFE9GTmwKhPkCR1lUrnmUTEWyPiUeCJ8v5FEXFYo5VJkjpG1ZMWrwWW\nARvK+18CPtVIRZKkjlM1TF7IzO8M3snM71OcCS9JUuUw2RIRBzB02d7jKU5YlCSp8tFcHwHuAiIi\nNgFPAe9tqihJUmepGib/nplviYheoD8zf9FkUZKkzlI1TL5AcRZ8X5PFSJI6U9Uw+X5E3Ao8DDw/\n2JiZKxqpSpLUUXY6AR8RbylvvoziWiYnUiylsgA4otnSJEmdYrSRyV9Q7N5aChARD2TmoubLkiR1\nktEODfbwX0nSqEYLk+HrcBkukqTtVD1pcZCLPEqStjPanMnbI+JHLfdfVd7vAQYy83XNlSZJ6hSj\nhUnUefGI+CTFkV/TgauAdcBtFNeM3wCckZn9EbEEOA/YBlyfmTdGxAzgZmB/iiPJlmbmk3XqkSQ1\nY6dhkplP7+oLR8RRwJsz8/CI2Af4FrAauC4z74iITwDLyvNXLgYOpTiHZV1E3ElxnfmNmbkkIt5J\nEUan7Wo9kqTmjHXOZCy+Bpxa3t4IzAYWAneXbfcAxwKHAesyc1NmPgc8BMwHjgHuLLddVbZJkiah\nqmfAj1lmbgX+s7x7NvAV4F2Z2V+2PQvsC8wFWpdp2a49M7dFxEBEzMzM59mJ3t457fsQU4D9UbAf\nhtgXQ+yL9mksTAZFxMkUYfJO4ActD410mPFY21+ir29z9eK6gP1R/MGwHwr2xRD7Ykg7QrXJ3VxE\nxLuAC4HjM3MT8MuImFU+PA94pvyZ2/K07drLyfie0UYlkqSJ0ViYRMQrgT8DTsrMn5XNq4DF5e3F\nwL3AI8AhEbFnROxOMTeyBrifoTmXRcCDTdUqSaqnyd1cpwG/Bnwp4sUjjM8EboiIc4CngVsy84WI\nWA7cR3FS5KWZuSkibgeOi4i1QD9wVoO1SpJq6BkYmFIntQ9Mpn2gy65+YKJLGJMVy4+e6BIa4b7x\nIfbFEPtiSG/vnNpLZTU6ZyJJ6g6NH8011XTaaEOSxoMjE0lSbYaJJKk2w0SSVJthIkmqzTCRJNVm\nmEiSajNMJEm1GSaSpNoME0lSbYaJJKk2w0SSVJthIkmqzYUecfFGSarLkYkkqTZHJnrRWEZoU/VC\nWpJ2jSMTSVJthokkqTbDRJJUm2EiSarNMJEk1WaYSJJqM0wkSbUZJpKk2gwTSVJthokkqTbDRJJU\nm2EiSarNMJEk1dboqsER8WbgLuDTmXltROwH3AZMAzYAZ2Rmf0QsAc4DtgHXZ+aNETEDuBnYH9gK\nLM3MJ5usV9W5wrCkVo2NTCJiNvBZYHVL82XAdZm5AFgPLCu3uxg4FlgInB8RewOnAxsz8wjgSuCq\npmqVJNXT5G6ufuAE4JmWtoXA3eXteygC5DBgXWZuyszngIeA+cAxwJ3ltqvKNknSJNTYbq7M3AJs\niYjW5tmZ2V/efhbYF5gL9LVss117Zm6LiIGImJmZz+/sfXt757TpE6hdJsO/yWSoYbKwL4bYF+0z\nkVda7GlT+0v09W3etWrUmIn+N+ntnTPhNUwW9sUQ+2JIO0J1vI/m+mVEzCpvz6PYBfYMxSiEkdrL\nyfie0UYlkqSJMd5hsgpYXN5eDNwLPAIcEhF7RsTuFHMja4D7gVPLbRcBD45zrZKkihrbzRURBwPX\nAK8HXoiIU4AlwM0RcQ7wNHBLZr4QEcuB+4AB4NLM3BQRtwPHRcRaisn8s5qqVc3yMGJp6mtyAv6b\nFEdvDXfcDrZdCawc1rYVWNpIcZKktvIMeElSbYaJJKm2iTw0WNqO8ytSZ3JkIkmqzTCRJNVmmEiS\najNMJEm1GSaSpNoME0lSbYaJJKk2w0SSVJsnLapjeYKjNHk4MpEk1ebIRF3BUYzULEcmkqTaDBNJ\nUm2GiSSpNudMpGGcX5HGzpGJJKk2w0SSVJu7uaQa3CUmFRyZSJJqc2QijRNHMZrKpmyYjOV/XElS\nPVM2TKRO5ihGncYwkTrcWILnnmtObrASdTMn4CVJtTkykbrIoo/cVXlbd59pLAwTSTvU1EEshtTU\nZJhIGlceXDA1GSaSJq2mgmcqB1rVz9buz9UzMDDQ1hdsp4j4NPBbwADwh5m5bpSnDPT1bQY8z0TS\n+JsMwbMrf/vuuebknrrvO2lHJhFxJHBQZh4eEf8FWAEcPsFlSdKIuvlL7GQ+NPgY4O8AMvMJYK+I\n2GNiS5Ik7cikHZkAc4FvttzvK9t+sZPn9PT2zgE8OUuSxtNkHpkMV3ufniSpGZM5TJ6hGIkMeg2w\nYYJqkSTtxGQOk/uBUwAi4jeBZzJz88SWJEnakcl+aPDVwDuAbcC5mfnYBJckSdqBSR0mkqTOMJl3\nc0mSOoRhIkmqbTKfZ1LZLiy7MiVExCeBBRT/jlcB64DbgGkUR76dkZn9EbEEOI9i7un6zLxxgkpu\nTETMAh4HLgdW06X9AFB+zv8BbAEuBr5DF/ZHROwO3ArsBbwMuBT4KfC/KP5WfCczP1Bu+zHg1LL9\n0sz8yoQU3WYR8WbgLuDTmXltROxHxd+FiJgB3AzsD2wFlmbmkyO9V8ePTFqXXQHOBj4zwSWNi4g4\nCnhz+bnfDfwFcBlwXWYuANYDyyJiNsUflGOBhcD5EbH3xFTdqIuAn5W3u7YfImIf4OPAEcBJwMl0\nb3+cBWRmHkVxZOj/pPj/5A8zcz7wyog4PiIOAP47Q332qYiYNkE1t035b/xZii9Xg8byu3A6sDEz\njwCupPjCOqKODxO6d9mVr1F8kwLYCMym+EW4u2y7h+KX4zBgXWZuyszngIeA+eNbarMi4jeANwJf\nLpsW0oX9UDoWWJWZmzNzQ2a+j+7tj38H9ilv70XxZeOAlj0Xg31xFPD3mfl8ZvYBT1P8PnW6fuAE\ninP2Bi2k+u/CMcCd5barGOX3YyqEyVyKpVYGDS67MqVl5tbM/M/y7tnAV4DZmdlftj0L7Mv2/TPY\nPpVcA3y45X639gPA64FXRMTdEbEmIo6hS/sjM/8GeF1ErKf48vVR4Octm0zpvsjMLWU4tBrL78KL\n7Zm5DRiIiJkjvd9UCJPhumrZlYg4mSJMPjTsoZH6YUr1T0S8F/h6Zv5whE26oh9a9FB8G38PxW6e\nm3jpZ+2a/oiI3wN+lJm/DhwNfH7YJl3TFyMY6+ffab9MhTDp2mVXIuJdwIXA8Zm5CfhlORENMI+i\nb4b3z2D7VHEicHJEfAP4feBP6M5+GPRvwMPlt9J/ATYDm7u0P+YD9wGUJzzPAn6t5fFu6otBY/l/\n48X2cjK+JzOfH+mFp0KYdOWyKxHxSuDPgJMyc3DieRWwuLy9GLgXeAQ4JCL2LI9umQ+sGe96m5KZ\np2XmIZn5W8ANFEdzdV0/tLgfODoidisn43ene/tjPcV8ABGxP0WwPhERR5SPv4eiLx4AToyImRHx\nGoo/pt+bgHrHw1h+F+5naF52EfDgzl54SpwB343LrkTE+4BLgO+3NJ9J8Qf15RSTiEsz84WIOAX4\nGMVhj5/NzC+Mc7njIiIuAZ6i+DZ6K93bD+dQ7PoEuILikPGu64/yD+MK4NUUh8//CcWhwX9F8UX6\nkcz8cLntHwBLKPriosxcvcMX7SARcTDFfOLrgReAf6X4jDdT4XehPKLtBuAgisn8szLzxyO935QI\nE0nSxJoKu7kkSRPMMJEk1WaYSJJqM0wkSbUZJpKk2qbEqsHqXhFxPHABxaqms4EfAudk5sYJLWwX\nRcRZwLGZ+XsNvsfbgZ9m5pMR8Y/AFZm5qqn3U3dwZKKOVa4T9HngtMw8KjMPpTjP5OydPlFLgQMn\nughNLY5M1MlmUYxGZg82ZOYfDd6OiLdQnLQ1o/z5UGZ+KyIOA66nWEV2NcWS7bMolrGfnpkXlc9/\nimKUsD4iPkFxZvAs4KsU1ws5ElgO/AR4E8WJYe/OzF9FxNnAB8q2BzPzjyNiL+BzQC/wSuCazPzr\nqh92F2q4EPhdiiVWvk2x1ND/pjir+dCIOL986WPK22+guJbH8DWspFE5MlHHKtcj+zjw7YhYFREX\nRkS0bPIF4P2ZuRD4IMXZvFAEzEXldS6+xyhfqiLiVGBeZh5Zjn5+neK6FwCHA39cXldmK/CucumO\nC4EFZftryrquAO7NzKMpVmy4LCJ6q3zWXajhIOD95WMnUlw8jsy8kyJYPpKZD5TP78nMEylGLC+G\nsTQWjkzU0TLzTyPiBuCdFNeleCQiLqD4Bh7AjS35skdE7Aa8leKbPRTrD43mKODwcn4BilHFARRX\nMHwiM58t258G9gYOAb45uPx3Zp4FL17Q7JCIOLPc/oXydVqX/25XDW+luEbFr8r3vgt42wivPfia\nPwH2rFCLtB3DRB0tIl6Rmf8BfBH4YkTcQTHy+GugvxyVDH9O61La21puD19baPDaDf0UlzL982Gv\ns5Di0ritesrX2dGovx/4YGY+urPPNIKx1rAbL/1sW3fy2q3P75bl19Vm7uZSxyqX4P96RMxpaT4Q\nWF/uAnsqIk4ot31DRFxcbvM4xSVaoVgNddAvgP3K7d8EvKpsXwu8JyKml49dXO5GGsk6ijmJPcrt\nv1QuureWYg6DiJgVEX85+JoVjLWG/wscXK6EOx347ZbHtlHMIUlt48hEHSsz74uINwCrI+JXFN+q\n/w04t9zkvcBnImI5xR/Pwasxfgz4y4j4GMXy44PuAJZGxBrgUeC7ZfvfUsw5PBwRW4F/Bp6kWKp8\nR3X9qFzBeFVEbAHWZuY3ywn9GyJiLfAyipHG8FEFwHEtu7OgOFjgi2Os4Tvlrq1HgR8Bj1Fcuhbg\nH4C/iojzdvRcaVe4arC6XkQMADNG+MPekcrRyFnAbZnZHxGfATZk5lUTW5mmKndzSVNQGYyvozgg\n4Wvl7esmtipNZY5MJEm1OTKRJNVmmEiSajNMJEm1GSaSpNoME0lSbf8f1yXGoB90r1EAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 75)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1000, 0, 5500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XLiHiIQVARYe"
   },
   "source": [
    "### We can see from the histogram how length of reviews is spread. We'll set up the max sequence length to 250 because most of the reviews are less than 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueENBNuHyOw_"
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjvKXOAL_m_j"
   },
   "source": [
    "Since we cannot store anything other than alphabets and numbers we need to clean the sentences and remove all the unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0fwfP-Q_yUN4"
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FD2ZZDRgPxha"
   },
   "source": [
    "**Now let's convert all the training files. We'll load in the movie training set and integerize it to get a 25000 x 250 matrix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the word matrix file it takes long time so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbrkgFc2yYHC"
   },
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in positiveFiles:\n",
    "   with open(pf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "\n",
    "for nf in negativeFiles:\n",
    "   with open(nf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "#Pass into embedding function and see if it evaluates. \n",
    "np.save('aclImdb/ids_Matrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O2dR_3mYyfTW"
   },
   "outputs": [],
   "source": [
    "ids = np.load('aclImdb/ids_Matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgZ-JW_MRY8V"
   },
   "source": [
    "These functions will create training batches and testing batches and when called will return the words with sentiment lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsFwK70byy6M"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxKgpYl7Rrh7"
   },
   "source": [
    "## Hyperparameters of Model\n",
    "\n",
    "\n",
    "these are some of the defined hyperparameters:\n",
    "* batch size, \n",
    "* number of LSTM units,\n",
    "* number of output classes,\n",
    "* number of training iterations. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgZ4xxEhEWr3"
   },
   "source": [
    "## Experiment 2 network configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpejjMFhy0C9"
   },
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "lstmUnits = 50\n",
    "numClasses = 2\n",
    "iterations = 120001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CG2CnzZ-y3Ui"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGxhq_iP6xTw"
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250#Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSHUAwzEFn4L"
   },
   "source": [
    "**Activation Function** - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YJNIEZO65xc"
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits,activation='relu',reuse=True)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fH64MxGF0NH"
   },
   "source": [
    "Network Initializer - xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfUcz12L7T0g"
   },
   "outputs": [],
   "source": [
    "init = tf.contrib.layers.xavier_initializer()\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]),initializer=init)\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o9M4EUxf7YJz"
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1v8lx2t8l4z8"
   },
   "source": [
    "* cost function - Cross entropy\n",
    "* Optimizer -  Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "7IMubUWqGMNC",
    "outputId": "6748751e-c986-42d8-e756-13d40acec47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\\noptimizer = tf.train.AdamOptimizer().minimize(loss)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "'''\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edrEDlNZ7bwd"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "sess = tf.Session()\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard_experiment_relu+sigmoid_costFunction/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEDA95Zr7gGE"
   },
   "outputs": [],
   "source": [
    "from tqdm import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckqnKroa73yU"
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#p_bar = tqdm(total=800)\n",
    "for i in tnrange(iterations):\n",
    "   #Next Batch of reviews\n",
    "   nextBatch, nextBatchLabels = getTrainBatch();\n",
    "   sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels},)\n",
    "   \n",
    "   #Write summary to Tensorboard\n",
    "   if (i % 50 == 0):\n",
    "       summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "       writer.add_summary(summary, i)\n",
    "\n",
    "   #Save the network every 10,000 training iterations\n",
    "   if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models_experiment2/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Trained model and create inference session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "fPJJ_b7874q7",
    "outputId": "0f8a16b1-463d-4107-ceb9-93d5af9668b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models_experiment2/pretrained_lstm.ckpt-120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models_experiment2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "OQI_EG6UEWBq",
    "outputId": "f4724ab8-8e5a-4137-bc78-a2011b8e2441"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 62.5\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 54.16666865348816\n",
      "Accuracy for this batch: 58.33333134651184\n",
      "Accuracy for this batch: 70.83333134651184\n",
      "Accuracy for this batch: 83.33333134651184\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Sentiment analysis model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](Images/accuracyModel2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss vs Number of Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](Images/LossModel2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen multiple components involved in sentiment analysis. When appropriate hyper-parameters are changed it changed the model training speed and performance of model. So finally we trained a model which is able to understand sentiments of movie reviews. \n",
    "\n",
    "The activation function function makes a big difference when it comes to training a network ReLu could not compete the accuracy of sigmoid function. ReLu takes a lot of time to train the network when compared to sigmoid.\n",
    "\n",
    "Gradient estimator : Adam, in most of the cases performs better than RMSProp. However, in this case adam could not perform well. Adam was training the network rapidly but the testing accuracy was low. RMSProp was much slower compared to adam but accuracy after training was much better than adam\n",
    "\n",
    "The loss functions did not make much difference in both of the experiments both of the loss functions were performing similarly.\n",
    "\n",
    "Number of epochs depends on the type of network you set up. If the activation functions and gradient estimators are not well tuned then the same model would take a lot of epochs to train on contrary if network is designed well less number of epochs can give you well trained model.\n",
    "\n",
    "The network initializers are used to define the initial values of the weights. There are many types of initializer available. The network worked best when the LeCunn uniform initializer was used."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_analysis_exp2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
